\chapter{Discussion}

Here we discuss our amazing results from the method, a lot of tables and stuff. Yay


\section{Performance Measure}%How we calculate precision/validation top k. Consequences etc
We are choosing top 1 user for now. A problem with this blablabla \todo{write more}
Maybe F1-score is not the right measure for this type of problem. Depends on the business-logic of the problem, if it is more tolerable to recommend too many users or too few. In that case maybe the precision or recall by itself would be more fair. 
As an alternative human-based evaluations might be more interesting to evaluate.

\section{Filter Bubble}
An upcoming issue with recommender systems in general is filter bubble.
Filter bubble is a consequence when training the networks on user data. The problem is that users will only be recommended similar information/items to what they have previously liked/encountered, meaning that the users will stay inside their bubble. 
\\\\
One of the simple countermeasures against this is if the users themselves are aware of this and actually browse information outside of their comfort zone. This additional information about users will hopefully be taken into account when training the network. 
\\\\
\todo{How we treat upvotes/downvotes as interesting, this is in method already. Discuss problems here}

\todo{consider if this is the right place for this}
Similar issues may arise if a user likes too general information. It will be hard to recommend specific things to that user, because a general view would result in all information being recommended to the user. This is more of an issue of the dataset that is being used for the training of the network. \todo{Discuss variance in our dataset here maybe?}
\\\\
%not having more personal data about the user
The Reddit dataset that is used in this project, and Reddit in general, does not hold any information about the users. This can be seen as a drawback compared to other platforms, for example Amazon where users have information about their interests, social media and occupation, but also a short bio. \todo{Maybe give source of Amazon.com}\\
The project does not cover data with access to personal information, in order to do this a new dataset has to be found or labeled manually and for now, will be seen as out of scope.

\todo{Discuss computing power restrictions}
\section{Application}
Due to time restrictions integration with an application was not developed
\section{Dataset}
This section will discuss the dataset, the disadvantages with it and what assumptions were made about it and how that might have affected the result.
\subsection{Size of dataset}
\subsection{Treating downvotes and upvotes as one}
In this thesis we made the assumption that both up-votes and down-votes were signs of interest. We also assumed that if a users did not interact with the content that users was not interested in that content. These are big assumptions that are not based on much more than intuition and might not be the best way to label the data. For instance it might be more reasonable to think of down votes as disinterests, upvotes as interests and not consider the neutral votes at all since the user might not even have seen the post. 
\section{Future work}
As this project was time bounded somethings was left untested, this section will discuss in what way this project can be extended.

\subsection{Individual networks for users}
One problem with the current implementation is that the maximum number of users that it can be used for without retraining is fixed. This makes this solution unfeasible as a commercial application. One way of counter this would be to train one network per user.
The pros for this solutions are (1) when our network were tested against Facebooks classifier on one user we beat them \ref{some section} (2) as mentioned in the paper \textbf{Multilayer Feedforward Networks With a Nonpolynomial Activation Function Can Approximate Any Function} by \textbf{Leshno, Lin, Pinkus, Schocken} a network that models a function $f:R^N->R^M$ can theoretically be seen as $M$ networks modelling $M$ functions $f_i:R^N->R$ \parencite{leshno1993multilayer} 

\subsection{Hyperparameters that were not considered}
Due to the time constraint we were forced to limit our development of the model and thus creating some constraints. This lead to that we were only able to create a subset of networks, i.e. in our networks all hidden units have the same number of neurons. This might not be the best way of modelling the networks, many networks instead follow the geometric pyramid rule \parencite{masters1993practical}, which suggests that number of neurons shrinks with each layer.

\subsection{Features that were not used}
Additional to the title and the subreddits there are more features to the reddit data that might have been used. One example of this the acctual content of the post. When creating a post it has to have title and it will posted on a subreddit, besides this the poster can also choose to add some content e.g. text or images or links. There might be some pattern between what type of content a post have and a users interest in that post.
\subsection{Using time}
It has been shown that it often is better to use the result from each unroll to make predictions (see chapter 10.2 in \parencite{Goodfellow-et-al-2016}), this might be in our case as well. Although we have a lacking knowledge about this and are not sure how to apply it to the problem, this was also discovered late in the project and there were not enough time to investigate and evaluate.

\subsection{Encoding of subreddits}
An improvement to investigate is to also use RNN for subreddits, hoping that the name of subreddit will play a significant role in the performance, the reasoning is that some subreddits have similar names (e.g machinelearning and learnmachinglearning) or have similar semantic meaning. This motivates using character-based RNN for the subreddits in the network.

\section{Future work}
Regarding the content of a post, some analysis on the dataset revealed that only about $x\%$ of all posts actually had any content - the rest only had a title. This lead to the decision to not create a new RNN input layer for parsing the content of a post. Instead the dataset was processed to only show whether a post had any content or not, in a binary way. This was also concatenated with the output of the existing RNN and the subreddit input. Just like for the subreddits; the option to either use the extra input or not turned into a hyperparameter.