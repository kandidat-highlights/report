\chapter{Discussion}\label{chap:discussion}

This chapter will discuss the obtained results from Chapter \ref{chap:results} and potential problems with the methodology leading to them.

\section{The Models and Their Performance on the Datasets}
The best performing network, as seen in Section \ref{sec:final_model}, got an $F_1$ score of $0.4548$ on the five users dataset. $F_1$ score is a combination of precision and recall. The precision for this network was $0.3552$ and the recall $0.6318$, which means that out of all recommended users about $64\%$ would not be interested, and about $37\%$ of the users that should get predicted were not. As one of the motivating factors for this thesis was to lessen the abundance of uninteresting notifications that users of social media get, the low precision can be seen as a poor result as it more often results in irrelevant notifications than not. However, the low density of user votes, as seen in Table \ref{table:5_user_density}, for a given post could mean that a lot of users have not seen a post and therefore not reacted to it. That they have not seen a post could still mean that the post is of interest to them, but that interest is not reflected in the dataset. A user study would potentially capture the interests of the users better than the dataset and therefore give a more accurate performance measure than just checking against a validation set. An example of this can be found in \cite{suorra2016assisting}.
\\\\
It is interesting to investigate the hyperparameters that were used and what conclusions we can draw from them. First of all, as seen in Table \ref{table:hyperparameters_final}, the best performing network did not use any pre-training or additional data (i.e subreddit as input). This could indicate that our assumption, that subreddits could be used to predict a user, was wrong. When looking at the top five performing networks that we tested for both the 5 and 50 users datasets, it seems that the choice of unit types in the recurrent neural network (i.e. \textit{LSTM} or \textit{GRU}) did not matter. Looking at the same top networks we can see that almost all of them use the dynamic prediction limit (predicting all users with an above average certainty). This is most likely because the ones that should be recommended have a higher probability and vice versa, the prediction boundary then cuts between these two groups in an efficient way.
\\\\
To test our assumption that hyperparameters that were tuned for a smaller amount of users would scale to more users, the model that performed best on the dataset with 5 users was also tested on the dataset with 50 users. The result of upscaling the number of users to predict but keeping the hyperparameters the same was disappointing. The new network performed poorly on the dataset that it was not designed for, as seen in Table \ref{table:final_all_results}. This suggests that it will not be possible to use a smaller network (in terms of users) in order to solve the problem with more users. As a consequence it will be hard to keep updating the model since a real application gets more users daily. The fact that it does not scale well is not that surprising in hindsight. More labels means a greater complexity so it is not certain that the degree of freedom in the model optimised for five users is enough, at the same time as a larger degree of freedom in the model for five users means that it overfits to the training data too quickly.
\\\\
Another assumption that was mentioned in \ref{sec:deciding_dataset} was if a user has voted on a post that indicates that the user found the post enjoyable or interesting, and a lack of interaction means that the user did not find it interesting. However, if a user has downvoted a post, that could potentially mean one of two things; either the user did not like the content and does not want to see similar posts again or the user finds the post interesting but does not agree with the point of view of the author. Similarly with no interaction, a user might simply not have seen the post, has not liked it, or has not cared enough to vote on it. There is no clear way of knowing what is wrong or right without talking to a large subset of the users.
\\\\
When comparing our model to the baselines, the $F_1$ score did surpass that of the N-grams based model on the 5 users dataset. The same is not true for Facebook's fastText classifier, the network is consistently worse than fastText (as can be seen in Section \ref{sec:baseline_comp}). These results were consistent regardless of whether the dataset was word stemmed or not. In its current state, with the model's $F_1$ score being comparable to that of the N-grams model and not better than that of Facebook's fastText, it is hard to motivate its use considering the time it takes to train. There are several reasons that could potentially explain why the network did worse than fastText and a few of them are discussed in Section \ref{sec:dataset}. One of these reasons could be that fastText only recommends the one user it is most certain of, while our model recommends all users over a certain threshold thus introducing a margin for error.

\section{The Datasets' Influence on the Final Results}\label{sec:dataset}
An interesting discovery made when examining the datasets was the scarcity of posts with more than one vote, especially in the downscaled five users dataset. As mentioned in Section  \ref{sec:five-user-data-set}, only about $1\%$ of titles in the training set for 5 users have more than one user that has voted on that title -- that is, only $1\%$ of the titles have overlapping users, and it is even less for the validation and testing sets. Having that few titles with user overlap could mean that the best way of choosing which users to recommend is to always pick the one or two most certain users. Another possibility seeing as there is little overlap between subreddits is to go for a purely statistical approach; given an input title and subreddit recommend the user that is most active in that subreddit. The problem with both of these approaches is that they are not very practical, in practice there is most likely more than one or two people that would like to get a recommendation seeing as how popular Reddit posts can get tens of thousands of votes.
\\\\
Another consequence of votes being distributed in the way just described is that recommending more than one user is most often wrong, as any model that recommends two or more users in that case will get at least one of them wrong. This will make it difficult to evaluate a model for a real world usage, which presumably does not share a similar distribution,  since the network should most certainly want to recommend more than one user for that case. Most users have very few votes in the majority of all subreddits they have voted in, this shows that users tend to be most active in just a small subset of subreddits. An example of this is the user \textit{akkartik}; in Table \ref{table:5_user_density} we can see that of the total $1387$ votes, $1333$ of these are from two subreddits. Even though not all users are as devoted to a particular subreddit, there is definitely a trend of keeping to a few subreddits and being mainly active in those. The fact that a user almost exclusively visits one, or a small number of, subreddits could result in a filter bubble, as described in \parencite{pariser2011filter}. A filter bubble occurs when a user only likes a few similar categories of data, a system that recommends new content will then naturally only recommend content from those categories. The aim of a recommender system is to recommend content that a user would not otherwise find, to then only recommend content from the same category is therefore not so useful as the user would likely find it anyway. Avoiding a filter problem is a complex task that was not the purpose of this project. It is still interesting to consider though and some of the work proposed in Chapter \ref{chap:future_work} could potentially help.
\\\\
Another interesting result was that the network trained on the 5 users dataset benefited from the word stemming while the network trained on the 50 users dataset did not. As stated in \cite{introduction2008Information} stemming only helps in cases where we have sparseness in the data.