\chapter{Discussion}\label{chap:discussion}

This chapter will discuss the obtained results from chapter \ref{chap:results} and potential problems with the methodology leading to them.

\section{The Models and Their Performance on the Datasets}
The best performing network, as seen in section \ref{sec:final_model}, got an $F_1$-score of $0.4548$ on the five user dataset. $F_1$-score is a combination of precision and recall. The precision for this network was $0.3552$ and the recall $0.6318$, this means that out of all recommendations a user would receive, about $65\%$ would be classified as not interesting and that about $37\%$ of the users that should get predicted were not. As one of the motivating factors for this thesis was to lessen the abundance of uninteresting notifications that users of social media get, the low precision can be seen as a poor result as it more often results in irrelevant notifications. However, the low density of user votes for a given post could mean that a lot of users have not seen a post and therefore not reacted to it. That they have not seen it could still mean that that post is of interest to them, but that interest is not reflected in the dataset. A user study would potentially capture the interests of the users better than the dataset and therefore give a more accurate performance measure than just checking against a validation set, an example of this can be found in \cite{suorra2016assisting}.
\\\\
It is interesting to investigate values of the hyperparameters that were used and what conclusions we can draw from them. First of all, as seen in table \ref{table:hyperparameters_final}, the best performing network did not use any pre-training or additional data (i.e subreddit as input). This could indicate that our assumption, that subreddits could be used to predict a user, was wrong and that it only skewed the model value to find a less optimal extreme point. When looking at the top five performing networks that we tested for both the five and 50 users datasets, it seems that the choice of RNN unit types (i.e. \textit{LSTM} or \textit{GRU}) did not matter -- however, because of this and the fact that GRU is faster than LSTM; GRU should most likely be preferred in this network. Looking at the same networks we can see that almost all of them use the dynamic prediction limit (predicting all users with an above average certainty). This is most likely because the ones that should be recommended have a higher probability and vice versa, the prediction boundary then cuts between these two groups in an efficient way.
\\\\
To test our assumption that hyperparameters that were tuned for a smaller amount of users would scale, the model that performed best on the dataset with 5 users was also tested on the dataset with 50 users. The result of upscaling the number of users to predict but keeping the hyperparameters the same was disappointing. The new network performed poorly on the dataset that it was not designed for, as seen in table \ref{table:final_all_results}. This suggests that it will not be possible to use a smaller network (in terms of users) in order to solve the problem with more users. As a consequence it will be hard to keep updating the model since the real application gets more users daily. Considering that the amount of possible labels that the network can predict scales with the number of users in the dataset. This non-scalability behaviour is not that surprising in hindsight. More labels means a greater complexity so it is not certain that the degree of freedom in the model optimised for five users is enough, at the same time a larger degree of freedom in the model for five users means that it overfits to the training data too quickly.
\\\\
When comparing our model, the $F_1$-score achieved did surpass that of the N-grams based model on both the 5 and 50 user dataset, with and without stemming. The same is not true for Facebook's fastText classifier, the network is consistently worse than fastText as can be seen in section \ref{sec:baseline_comp}. In its current state, with the model's $F_1$-score being negligible to that of the N-grams model and not better than that of Facebook's fastText, it is hard to motivate its use considering the time it takes to train. There are several reasons that could potentially explain why the network did worse than fastText and a few of them are discussed in section \ref{sec:dataset}. One important aspect to keep in mind when comparing fastText to our network is that they do not use the same strategy for recommending users. The network recommends all users over a certain threshold while fastText recommends the one user it is most certain of.

\section{The Datasets Influence on the Final Result}\label{sec:dataset}
A very interesting discovery made when examining the datasets was the scarcity of posts with more than one up- or down-vote, especially in the downscaled five users dataset. As mentioned in section  \ref{sec:five-user-data-set} only about $1\%$ of titles have overlap in the training set for 5 users and it is even less for the validation and testing sets. Having that few titles with user overlap could mean the the best way of choosing which users to recommend is a top-2 or top-1 approach. Another possibility seeing as there is little overlap between subreddits is to go with a purely statistical approach; given an input title and subreddit recommend the user that is most active in that subreddit. The problem with both of these approaches is that they are not very practical, in practice there is most likely more than one or two people that would like to get a recommendation seeing as popular Reddit posts can for example can get tens of thousands of votes.
\\\\
Another consequence of votes being distributed in the way just described is that recommending more than one user is most often wrong. Any model that recommends two or more users will get at least one of the recommendations wrong. This will make it difficult to evaluate a model for the real world usage since the network should most certainly want to recommend more than one user, but each title in the dataset only has one label in most cases. Users tend to have few subreddits that they are most active in, i.e. many votes, and many subreddits that they have few votes in. A clear example of this is the user \textit{akkartik}; in table \ref{table:5_user_density} we can see that of the total $1387$ votes, $1333$ of these are from two subreddits. Even though not all users are as devoted to a particular subreddit, there is definitely a trend of keeping to a few subreddits and being active mainly in those. The fact that a user almost exclusively visits one, or a small number of, subreddit(s) could result in a filter bubble, as described in \cite{pariser2011filter}. A filter bubble occurs when a user only likes a few similar categories of data, a system that recommends new content will then naturally only recommend content from those categories. The aim of a recommender system is to recommend content that a user would not otherwise find, to then only recommend content from the same category is therefore not so useful as the user would likely find it anyway. Avoiding a filter problem is a complex task that was not necessarily the purpose of this project, it is still very interesting to consider though and some of the work proposed in Chapter \ref{chap:future_work} could potentially help.
\\\\
Another interesting result was the both networks were that both benefited from having the datasets stemmed. This confirmed one of the suspicions mentions in Section \ref{sec:enhacing_the_model} that increasing the frequency of shared words is a better indicator than the information that is lost by the stemming, such as word inflections. As stated in (Stanford INSERT) word stemming will only help in situations where we have sparseness in the data. The signficant increase in all of our models indicate that this is the case.
