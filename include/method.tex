\chapter{Method}%Är metod och utförande samma sak?
This chapter will describe the workprocess of this project and what has been done.
\section{Deciding on a dataset}
The first step to training a artificial neural network is to find a good source of data to learn from. Since ANN is a supervised learning technique it saves a lot of time if the data is labelled when fetched or that it automatically can be labelled by some relatively simple computer program. A dataset that is useful for ANN is very large, a small data set might range from 30 000 entries corresponding to 3 0000 labels, finding those labels manually would be extremely time consuming.

We considered two datasets, Ubuntu Dialogue Corpus and Reddit data. The ubuntu set contained forum posts with back and forth discussion whereas the reddit set contained only the original post but had the possibility to extract which users up voted and down voted that post. The fact that the reddit set had up-votes and down-votes lead to the choosing of that dataset since these votes could be seen as labels and thus be used when classifying. 

\subsection{Upvotes and downvotes}
The network is learning to predict which reddit users are interested in a given title of a reddit post. It learns this by looking at the interactions that users have had with similar posts. If user have upvoted a post that clearly means that the user found the post enjoyable or interesting, but what if the user has downvoted the post or not interacted at all with it? %kan omformuleras efter att vi har bestamt om vi ska ha med dem eller ej.
If a user have downvoted a post that could mean one of two things; either the user did not like the content and do not what to see similar posts again or the user find the post interesting but does not agree with the point of view of the poster, in this case the user most likely want to see more similar posts. Similarly with no interaction, a user might simply not have seen the post not liked it or not cared enough to upvote it. This is a problem of social studies, how do user use their downvote?

\section{Gathering data}
något om att deep ann:s behöver mycket data %Must be mentioned in theory part
\section{Modeling as an RNN}
Since the input to the network is text it was decided that the initial layer would be a recurrent neural network. This was because RNNs have the capability of keeping context through the input and context is very important in natural language. The RNN would then provide a ANN with context free input that the ANN could generalise from.
\subsection{Tensorflow}%The explanation about tensorflow should be made in terminology, here we shall only name how we will use it.
Tensorflow is a high level framework, developed by Google to give the possibility of developing machine learning applications without having a deep knowledge. %även om det är sant att man inte behöver vara ML proffs tycker jag det får oss att låta dumma% 
\subsection{LSTM-network}
\subsection{Scaling down}
As by suggestion from our supervisor we will begin with a very small scale network, instead of having the number of users in the tens of thousands we will start with 2-5 users. The reasoning for this is it that it will now be easier to not just train the network but to also to analyse it. This is a common approach in machine learning (source Olof) and the hope is that whatever model works on the small scale will hopefully continue to perform when it is scaled up.    
\subsection{Overfitting on purpose}
Overfitting is something you want to avoid in the final model but can be useful in development. Overfitting is a result of having learnt the training data too well but the key here is that something has been learnt. If the model has learnt something you at least know you are on the right path, it is not making guesses at random. Overfitting is therefore the first milestone for our scaled down network. 
\subsection{Tuning hyperparameters}
\subsection{Scaling up}
\section{Baseline}
When deciding how well a model performs it is compared against a baseline. The baseline puts the accuracy of the model into some context. You might have an accuracy of 90\%, is that good or bad? You need one or more baselines to decide this. 
\subsection{Random classifier}
A random classifier is a model that given an input $x$ selects one of $n$ output values uniformly at random. This results in an expected accuracy of $\frac{1}{n}$. This is a baseline that any real life model needs to beat with confidence.
\subsection{Text classification using an N-gram}
blabla

\subsection{Collaborative filtering}
%Flytta förklaringen till ordlista? 
Collaborative filtering can be used in what is called recommender systems. The goal of a recommender system is to recommend users products that the user finds interesting or helpful. Collaborative filtering operates under the assumption that if Bob likes cats and dogs and Alice likes cats, Alice is more likely to also like dogs since Bob likes dogs and Alice share a interest with Bob. Collaborative filtering has found success in recommending items such as movies (netflix prize source). %Kanske något mer om hur vi använde detta som baseline%

\subsection{N-gram}
\section{Computing power restrictions} %Should 
\section{Integration with an application}


%Saker som man är också intressanta: Kolla upp variansen, debugg, hur vi hitta felet etc. 