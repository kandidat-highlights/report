\chapter{Method}\label{chap:method}
This chapter describes what has been done and discusses the decisions taken.

\section{Deciding on a Dataset}\label{sec:deciding_dataset}
As described in the initial problem description, deciding on a dataset was essential for the project to proceed. A first step was thus to find a dataset which satisfied the constraints presented in Section \ref{sec:select_dataset}. This was needed as a first step since an artificial neural network needs a source of data to learn from. Since artificial neural networks are mostly used for classification, the data used to train it must be labelled. It thus saves a lot of time if the chosen dataset is already labelled when fetching it, or that it can be automatically labelled by some automated process. Manually labelling several thousands of data points was never an option when deciding on the dataset. It would be too time consuming for the scope of the project.
\\\\
After searching for different sources of data it finally came down to two datasets; Ubuntu Dialogue Corpus \parencite{lowe2015ubuntu} and a dataset based on posts with corresponding user votes from Reddit (see appendix \ref{appendix:reddit}). The Ubuntu Dialogue Corpus contained forum posts with back and forth discussions, whereas the Reddit dataset contained posts and whether certain users had up- or down-voted those posts. Using the Reddit API (\url{https://www.reddit.com/dev/api/}) it was also possible to extract more information from a post, such as which subreddit it belonged to.
\\\\
The reason for having the Ubuntu Dialogue Corpus and the Reddit up- and down-votes data as final considerations comes down to a few factors. Most important were the overall characteristics of the data, i.e. the data was text content written by users for other users. There was also interaction with the content which could be used to indicate interest in a topic. The availability of the data, its format, and its size were also factors.
\\\\
After comparing the two datasets the Reddit dataset was chosen. It had a simple format and primarily it had a stronger indication of user interest compared to the Ubuntu Dialogue Corpus. In the Reddit dataset, content was directly linked to users on the platform which had either up- or down-voted it. On Reddit, up- or down-voting content means that the user clicks a button indicating if they like or dislike a certain post. An assumption was made that the active decision to either up- or down-vote some content would indicate interest and that an average user is less likely to engage in a full discussion (as in the Ubuntu Dialogue Corpus) compared to just clicking a button. Given time constraints, the thesis only evaluates the case with the assumption that all votes, positive or negative, represent interest in the content and a lack of interaction with a post is interpreted as disinterest in it.
\section{Gathering Data}\label{sec:gathering_data}
The Reddit dataset is not so useful in its raw form. The raw data only consisted of ID references, as seen in Table \ref{table:raw_reddit_data}. This data had to be converted for this project, and the result can be seen in Table \ref{table:example_reddit_data}.
\begin{table}[h!]
    \centering
    \begin{tabular}{ c c c } 
        \hline
        \textbf{Username} & \textbf{Post ID} & \textbf{Vote} \\
        \hline
        \hline
        2bornot2b & t3\_89ko9 &-1\\
        \hline
    \end{tabular}
    \caption{An example data point in the raw Reddit dataset showing that the user 2bornot2b has downvoted the post with ID t3\_89ko9.}
    \label{table:raw_reddit_data}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabu}to 1\textwidth{ X[c] X[c] X[c] X[c] X[c] } 
        \hline
        \textbf{Title} & \textbf{Content} & \textbf{Subreddit} & \textbf{Upvotes} & \textbf{Downvotes} \\
        \hline
        \hline
        &&&& \\
        A Graph of NP-complete Problems & \textit{<empty>} & compsci & izzycat, cypherx, HattoriHanzo & \textit{<empty>}\\
        &&&& \\
        \hline
    \end{tabu}
    \caption{An example data point in the processed Reddit dataset showing information about a post (with no content) and which users showed interest in it.}
    \label{table:example_reddit_data}
\end{table}
\\
In the raw form shown in Table \ref{table:raw_reddit_data} the data is modelled as a relation from a specific user to a specific post. A post is also represented as a single ID instead of its title, content and subreddit. To get from the raw format to the processed format two steps were made.   
\\\\
Firstly, the ID of a post was used to retrieve more meaningful content from the Reddit API. This included getting a post's title, subreddit and eventual content (a lot of posts only had a title with no other content). As a post ID is only a reference in Reddit's database, the API was used to extract more information by passing the ID to it. Specifically, the API had an endpoint that could take one or more post IDs as input and return a list with all information available about the posts with the specified IDs. This enables the extraction of the posts' titles and content as well as which subreddit they belong to. As the dataset is large it would have been ineffective to make the extraction from the post IDs manually, therefore the extraction task was automated. The source code for this automation program is available at \url{https://github.com/kandidat-highlights/reddit-scraper}.
\\\\
Secondly, the data were re-arranged to model a relationship from a unique post to several users. In the original format each data point models a single user's vote for a post. A data point in the new format contained the information of a post and all users who have voted on that post. The final format, as shown in Table \ref{table:example_reddit_data}, was achieved by loading all of the data into a relational database. By having the data in a relational database, it was easy to extract different samples of data suitable for the experiments performed. As an example, it could be used to split the dataset into training, validation and testing subsets. It could also easily be analysed from a statistical standpoint in order to find patterns that could impact the result.

\section{Modelling the Artificial Neural Network}\label{sec:modelling_the_ann}
The general characteristic of the problem is that given some post, one or more users should be recommended based on their previous interest in other posts. A Reddit post in the given dataset is described by three properties; a title, some content and a subreddit (category). This means that the artificial neural network model should be able to use natural language as input. This property of the problem motivates the use of a recurrent neural network (see Section \ref{sec:rnn}) to fully capture the input of natural language. 

\subsection{A Basic Starting Point}\label{method_basic_start_point}
An initial model was thus to have an artificial neural network with a recurrent neural network as input layer, no hidden layers and an output layer. For the output layer, each user had to be represented in a way that made it possible to compare them against each other in terms of how likely they were to be interested in a given input post. In practice this meant that the output layer should scale the output of the model between $[0,1]$, as this allows for interpretation as probabilities. Two activation functions which do this were considered; the softmax function (see Section \ref{sec:softmax_function}) and the logistic function (see Section \ref{sec:sigmoid_function}). As the problem is to recommend one \textit{or more} users it can be seen as a multi-label classification problem, which meant that using the sigmoid activation function would likely perform better. However, at this point, none of the activation functions were ruled out.
\\\\
An initial model like the one just described was implemented using the machine learning framework \textit{TensorFlow} \parencite{tensorflow2015whitepaper}. For the recurrent neural network, LSTM units were used. At first the network was also modelled to only use the titles of posts as input since this was easier than using all of a post's features. The input layer was modelled to let one LSTM unit take one word, from the input sentence, at a time as input. The number of LSTM units was chosen by analysing the data in order to capture large amount of titles. However, it is still possible to choose any positive number of LSTM units -- hence the number of units became a hyperparameter. The words were transformed to indices, where each index corresponds to a row in the embedding matrix, as described in Section \ref{sec:rnn_nlp}.
\\\\
A problem that quickly revealed itself was the size of the dataset. As the dataset is large, training of the network was very slow even in this simple form. A natural next step was thus to scale down the dataset to allow faster iteration of models. Instead of having a dataset with several thousands of users the dataset was downscaled into only having five users and the posts that they had interacted with. By doing the downscaling it was faster to train the network, which in turn meant that it was faster to get a result and therefore more experiments could be made. The argument for using this strategy is the assumption that a model performing well on the downscaled dataset will continue to perform well as the dataset is upscaled again. 
\\\\
The goal with this simple artificial neural network model was to establish some starting point to assure that the basic concept was working. To test that the network was actually learning anything at all we looked for overfitting (see Section \ref{sec:overfitting}). If the model could not overfit on the training data using the downscaled dataset it would indicate some conceptual error or misinterpretation of the problem. However, once we had verified that the model did overfit, and thus was able to learn from the data, the work moved on to increase the accuracy of the model.
\\\\
At this stage, both the softmax and the logistic activation functions have been used to test the model. With the softmax as the activation function users were picked based on a hyperparameter $k$, where $k$ was the number of users that would get predicted. This meant that the top $k$ users with the highest probabilities would always get picked as predictions from the model. In practice this meant that even if only a single user should have been predicted, the model would always predict $k$ users even if the $k-1$ remaining users should not be predicted at all. The reason for using this method to pick users is based on how the softmax function normalises all probabilities. It is not practical to have a condition to pick all users with a probability higher than a certain constant, since the constant will depend on the number of users. This was however supported by the logistic function, which in the end resulted in the softmax function being removed from further testing in the output layer. The limit of when to pick a user or not given a probability introduced a new hyperparameter based on how to set the limit. One option considered was to have a constant limit $x$, e.g. picking all users with a probability above $x=30\%$. Choosing $x$ then becomes a problem of optimisation. Another option considered was to pick all users with an above average probability. The hyperparameter then becomes to choose which of the two options to use and to choose a value of $x$ if the first option is used.
\\\\
It is common to use $F_1$ score (see Equation \ref{eq:f1score}) when evaluating machine learning models \parencite{yang1999re}. A benefit of using the logistic function over the softmax function was to measure more meaningful performance. The softmax function was not practical because a constant number of users would always get picked when making recommendations. During the performance testing of the network, users were selected based on the probability that the network assigned to them if that probability exceeded a certain threshold. With the more dynamic way to predict users that the logistic function allowed, the recall could be calculated in a more meaningful way which in turn allowed to calculate the $F_1$ score of the model. With the logistic as the output function precision, recall and $F_1$ score was measured and used to evaluate the performance of the model. In order to visualise the performance a TensorFlow tool called \textit{TensorBoard} was used.
\begin{equation}
    \label{eq:recall}
    recall = \frac{true\ positives}{true\ positives + false\ negatives}
\end{equation}
\begin{equation}
    \label{eq:precision}
    precision = \frac{true\ positives}{true\ positives + false\ positives}
\end{equation}
\begin{equation}
    \label{eq:f1score}
    F_1\ score = 2 * \frac{precision * recall}{precision + recall}
\end{equation}
\subsection{Enhancing the Model}\label{sect:enhacing_the_model}

\label{sec:enhacing_the_model}
After the performance measures were implemented to calculate precision, recall and $F_1$ score it became possible to accurately compare models against each other. Because of this, the model could now be enhanced and the changes could be evaluated. An enhancement that was easy to implement was regularisation: Both $l^2$ regularisation and Dropout (see Section \ref{sec:regularisation}) was added to the model. With regularisation two new hyperparameters were introduced; the dropout probability, $P_{dropout}$, and the $l^2$ factor, $\beta_{l^2}$. The options to use either $l^2$ or Dropout regularisation (or both) also became hyperparameters in themselves.
\\\\
In order to add more degrees of freedom to the artificial neural network, support for hidden layers was added to the model. The model was implemented so that any non-negative number of hidden layers could be used. For the layers' activation function the ReLU function, described in Section \ref{sec:relu}, was used. By adding hidden layers to the model two new hyperparameters were introduced; the number of hidden layers and how many neurons the layers should have. We simplified the model so that all hidden layers have the same number of neurons. This was to keep the number of hyperparameters, and thus the number of combinations, down to make experimentation easier. Each layer is connected as a chain in a fully connected way, so there are no parallel hidden layers and all nodes between two consequent layers are connected. This means that the output from the recurrent neural network goes to the first hidden layer and the output from that layer then goes to the second layer and so on -- until the output of the last hidden layer goes to the output layer. This is illustrated in a simplified way in Figure \ref{fig:ann_hidden_layers}.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\textwidth]{figure/method/ann_hidden_layers}
    \caption{A graph visualisation of an ANN with two fully connected hidden layers.}
    \label{fig:ann_hidden_layers}
\end{figure}
\\\\
Another extension of the model was to implement pre-trained word embeddings. The word embeddings would be pre-trained with GloVe \parencite{pennington2014glove}. Reason for using word embeddings is that they better capture the relation between words in a language \parencite{mikolov2013linguistic}, GloVe has also been shown to perform better compared to other methods for creating word embeddings such as word2vec \parencite{pennington2014glove}. The first step was to evaluate the performance of the already pre-trained embeddings that can be found at \url{https://nlp.stanford.edu/projects/glove/}. The two specific embedding matrices that were evaluated are pre-trained on "Wikipedia 2014 + Gigaword 5"- and "Twitter"-datasets. For these embedding matrices, different sizes were tested to find the one that performed best for our task. The second step was to pre-train a new word embedding matrix based on the Reddit dataset. The last step is to compare the accuracy between the embeddings matrices that were trained on "Wikipedia 2014 + Gigaword 5"- and "Twitter"-datasets against the embedding matrix trained on the Reddit dataset. The comparison was completed early in the project and the tests showed clear results of the Reddit's embedding matrix outperforming other embeddings.
\\\\
The reason for creating new embedding matrices was that some words can be community- or platform-specific, meaning that there was a chance of capturing more relations between the words with this method. Different dimensions of the embedding matrix were evaluated against each other. 
\\\\
To reduce the number of potentially irrelevant details in the data, such as word inflections, a technique called word stemming was used. The goal was to increase the number of shared words between the titles. This could lead to the classifier having an easier time to learn similarities in titles. By applying the Porter stemming algorithm \parencite{porter1980algorithm}, word inflections in the dataset was reduced to their base form. By doing this, the number of unique words are decreased at the same time as word occurrences increase. The potential consequence of using the stemming is that information is lost, information that might be important for making good predictions. This motivates evaluating the network by using both the original and stemmed dataset in the project.
\\\\
So far only the title of a post was used as input to the model. Since there is more data available about each post, such as its content and which subreddit it belongs to, the model was also enhanced to incorporate this. For the subreddits, the actual meaning of the subreddit's name (i.e. the actual text, e.g. \textit{politics} or \textit{humor}) was not of interest. Instead a post's subreddit was encoded as a one hot vector. This vector was fed through a network layer as a mean to re-size the input's dimensions and was then concatenated directly with the output from the existing recurrent neural network. The option to either use this extra input or not turned into another hyperparameter.
\\\\
Early in the project subreddits were tested as a classification target instead of users. The network showed good results for predicting which subreddit a post had been posted in. This meant that it could be useful to first train network at predicting subreddits and then use the weights from this network as initial weights when training on users. The reasoning for this was that if the network is already trained to distinguish between posts in a broad term (i.e. in terms of which subreddit they belong to), that could potentially be a good starting point for distinguishing between users' interest. Combining this strategy with using both the title and subreddit of a post as input as previously mentioned was also evaluated.

\subsection{Tuning Hyperparameters}
With the implementation of the model being complete, experimentation and tuning of the introduced hyperparameters was the next step. Hyperparameters are both actual parameters introduced as well as whether to use a certain feature of the model or not. For example, a model with $l^2$ regularisation turned off is different from one with it turned on -- it is not certain that the model will perform better with it turned on. All of the hyperparameters introduced are described in Table \ref{table:hyperparameters}.
\begin{table}[h!]
    \centering
    \begin{tabular}{ r  p{7cm} }
        \hline
        \textbf{Hyperparameter}  &  \textbf{Description} \\ \hline \hline
        Learning rate & A real valued, positive number scaling the network's gradient during training  \\ \hline
        Batch Size & Positive Integer specifying the size of each batch \\ \hline
        RNN units & Positive Integer specifying the number of units in the RNN  \\ \hline
        Embedding Size & A positive Integer specifying the size of each word representation in the embedding matrix \\ \hline
        Pre-trained embedding matrix & A binary choice to either use a pre-trained embedding matrix or not \\ \hline
        Trainable embedding matrix & A binary choice to either allow or disallow the embedding matrix to be updated during training \\ \hline
        Hidden layers & Non-negative Integer specifying the number of hidden layers in the model \\ \hline
        Neurons in hidden layers & Positive Integer specifying how many neurons a particular layer has \\ \hline
        Use $l^2$ regularisation & A binary choice to either use $l^2$ regularisation or not \\ \hline
        $l^2$ factor & A real valued scaling factor for the $l^2$ regularisation \\ \hline
        Dropout regularisation& Regularisation technique where some neurons sometimes are deactivated \\ \hline
        Dropout probability & The probability that any given neuron will kept \\ \hline
        Use constant prediction limit & A binary choice to either use a constant or average based prediction limit \\ \hline
        Constant prediction limit & A threshold such that if it surpassed a recommendation is issued  \\ \hline
        Use subreddit input & A binary choice to either use or not use the given posts subreddit as additional input \\ \hline
    \end{tabular}
    \caption{Table of all hyperparameters present in our model along with a description for each of them}
    \label{table:hyperparameters}
\end{table}
\\\\
As there are a lot of different combinations of hyperparameters, a dynamic and configuration based model was implemented. With this implementation the whole model can be defined using a single configuration file. The configuration file specifies all of the hyperparameters and when the model is ran it dynamically built an artificial neural network model to match them. This setup also enabled scheduling the training and logging the progress of different models in an automated way which made experimentation easier. Source code for the artificial neural network model can be found at \url{https://github.com/kandidat-highlights/model}.
\\\\
The goal of tuning the hyperparameters was to maximise the $F_1$ score for the validation dataset. Under normal circumstances an optimiser would likely be used to find the optimal parameters that maximised the $F_1$ score, but since the model is in fact an artificial neural network the time it takes for training the model makes an optimiser impractical. Instead, the optimisation relies on systematic testing of different combinations of hyperparameters. Different sets of hyperparamaters were randomly generated within some bounds. This has been proven, empirically as well as theoretically, to give better results instead of doing exhaustive search over all possible combinations. \parencite{bergstra2012random}
\\\\
The typical workflow when optimising the model was thus to look at a previous model that had performed well, identify a hyperparameter that should be changed, test the network with the updated hyperparameter and then compare the results. Which hyperparameter that should be changed could be chosen on a few different basis. For example, with the boolean hyperparameters (i.e. use dropout or not) it is easy to make sure both combinations are tested and if one hasn't been tested it is a good hyperparameter to pick.
\subsection{Upscaling}
After no further improvements could be made, in terms of $F_1$ score, on the downscaled dataset, it was time to scale up and evaluate it on a larger dataset with more users. The count of 50 users was selected for the upscaled model. 

\section{Comparing Against a Baseline}
When deciding how well a model performs it is compared against a baseline. The baseline puts the accuracy of the model into some context and shows how it measures against other models. In order to have a fair comparison, the users selected are chosen in a similar way as for the artificial neural network model mentioned in Section \ref{method_basic_start_point}. 

\subsection{N-grams Based Model as a Baseline}
In this project, a model based on N-grams \parencite{cavnar1994n} was used as a baseline, the model used was inspired by to those in \parencite{miao2005document}, and \parencite{khabiacluster}. Each user is represented by a vector based on the titles they have voted on in the training set. When the model receives a new title, the title is translated into a vector and cosine similarity \parencite{steinbach2000comparison} is used to measure which user the new vector is most similar to. The user whose vector is most similar to the new title's vector is recommended. For more details on implementation, see \url{https://github.com/kandidat-highlights/Ngram-baseline}.

\subsection{Facebook fastText as a Baseline}\label{method:fasttext}
Facebook has implemented an efficient sentence classifier called \textit{fastText} (\url{https://github.com/facebookresearch/fastText}). They used the new bag of words techniques mentioned in \parencite{joulin2016bag} in order to achieve fast classification while still maintaining accuracy.
\\\\
The aim of the comparison against fastText is to achieve similar if not better results in terms of accuracy. fastText only allows choosing a constant top $k$ users for the performance measure. In order to compare it to the ANN model, the best $F_1$ score was chosen from testing different values of $k$. Values for $k$ between $1$ and $n$, where $n$ is the number of users in the dataset, were tested. The best $F_1$ score from fastText is then compared to the $F_1$ score of the network model.
The performance in terms of speed will not be compared. When running fastText, some parameters can be set to specify e.g. learning rate. These parameters are further explained in fastText's GitHub repository at \url{https://github.com/facebookresearch/fastText}.