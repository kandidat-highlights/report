\chapter{Future Work}\label{chap:future_work}
During the work with this thesis a lot of different hyperparameters and methods were tested, but as time was a serious constraint there are still some interesting areas left untested. This chapter discusses areas to focus on in future work based on this thesis.

\section{Improving the Model Architecture}\label{sec:improving_arch}
A problem with the current choice of model is that the maximum number of users that it can predict without having to be re-trained is fixed. This makes this solution unfeasible in a commercial application with a growing number of users. One way to counter the problem of a fixed number of users would be to train one network per user. This solution should work just as well as the model presented in this thesis as a network that models a function $f:R^N->R^M$ can theoretically be seen as $M$ networks modelling $M$ functions $f_i:R^N->R$ \parencite{leshno1993multilayer}.
\\\\
A constraint introduced in order to simplify hyperparameter searching was to limit the number of neurons in the hidden layers to be the same for each layer. This might not be the best way of modelling the networks. Many networks instead follow the geometric pyramid rule, which suggests that the number of neurons should shrink with each layer \parencite{masters1993practical}. However, as some of the best performing networks tested had only zero to one hidden layer, it is not obvious that this extension would help -- unless the reason for the worse performance of the models with more hidden layers is because the geometric pyramid rule was not followed. Nonetheless it could be worth examining.
\\\\
Another constraint for simplification is that the ReLU function is exclusively used as activation function in the hidden layers. A problem with the ReLU function however is that something called dead neurons can occur. This is common when the ReLU function is used as it evaluates to zero when the input is less than or equal to zero. There is however a way to reduce the number of dead neurons with something called Leaky ReLU. Leaky ReLU is another function that uses a small negative slope instead of just being zero for negative input \parencite{maas2013rectifier}. A proposal is to try Leaky ReLU or other activation functions as alternatives to just using ReLU.

\section{Re-modeling Input and Output}
If work with the Reddit dataset is continued, there are a few things to consider. Firstly, a potential improvement for the input to the network is to use a separate recurrent neural network for the subreddits when using both titles and subreddits as input. This assumes that the name of a subreddit will have an impact on the model performance. The reasoning is that some subreddits have similar names (e.g \textit{machinelearning} and \textit{learnmachinelearning}) or have similar semantic meaning. This motivates using a recurrent neural network  with character-based vectors, as opposed to word-based vectors for the title input.
\\\\
Additional to the title and the subreddit, there are more features to the Reddit data that have not been used. One example of this is the actual content of a post. When creating a post on Reddit, it must have a title and must belong to a specific subreddit, beside this the author can also choose to add some additional (optional) content. This extra content could be text, images, or hyperlinks. The intuition behind using this extra information is that there might be some pattern between what type of content a post has and a user's interest in that post. There is a drawback with using the content of posts however; some analysis on the dataset revealed that only a small percentage of the posts had any extra content at all -- the rest only had a title and a subreddit.
\\\\
The problem with the model having a maximum number of users it can recommend, as described briefly in Section \ref{sec:improving_arch}, could also be solved by re-modeling the network's output. Instead of recommending individual users the network could potentially be used to recommend more general \textit{user types} or \textit{user profiles}. User profiles could be constructed from features of the users and the model would then try to recommend which user profiles would be interested in a certain post. 