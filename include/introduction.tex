% CREATED BY DAVID FRISK, 2016
\chapter{Introduction}
This is the report for the bachelor’s thesis “\varthetitle“ given by the Department of Computer Science and Engineering at Chalmers University of Technology.

\section{Background} \label{Section_ref}
In today’s digital age it can be a hard time keeping track of all the content and conversations on different social platforms; such as Slack, Facebook Messenger, WhatsApp and Reddit. People do not want to miss the conversations or news that might interest them, rather get instant feedback when these conversations or news take place. There is a problem with this in that people can not monitor all content at all times. In addition, it is an inefficient use of a person’s time to consume content that is of disinterest.
\\\\
On today’s social platforms notifications are indiscriminate in that it is hard getting notified when something of interest is posted while not being notified at all times. Some applications have tried to solve this by allowing manual “mentioning” or “highlighting” of a person to get their attention; Slack is one example of this \parencite{slack}. However, these functions put a constraint on the users where they need to figure out by themselves who might be interested in some content. It is also deemed as bad manners to highlight a person - or a group of people - too often. Studies have even shown that an excessive amount of notifications can lead to stress \parencite{relaffinity}. One solution to this problem could be some middle ground, such that content could be automatically highlighted for specific users when deemed interesting to them. Such a middle ground would be beneficial for both the users and the owner of the platform. It would make it easier for users to find interesting content which would improve the overall experience for the average user - this, in turn, benefits the owner of the platform as its users get more satisfied overall.
\\\\
Similar works have been done in other fields like electronic commerce. Amazon uses a technique called recommender systems to recommend new items for their users \parencite{amazonfiltering}. Their recommender system is not based on deep learning but rather compares the similarity between new products and products that a user has previously bought or rated \parencite{amazonfiltering}.

\section{Purpose}
The aim of this project is to evaluate the performance of using deep recurrent neural networks to do personalised recommendations of content for users on social platforms. By using a neural network trained for this task an application should then be able to use its predictions to individually notify users when, to them, interesting content is published.

\section{Problem Description}
To complete the purpose of the project the correlation between some content on the platform, and a users' interest needs to be modelled. This will be attempted using a artificial neural network. In order to do successfully model this problem there are a number of factors that must be considered. First of all, in order to use an artificial neural network a dataset which can be used for training needs to be acquired (SOURCE?). When a dataset is obtained, the ANN must be modelled to capture the relation between the contents of a text and a users interest. The modelling is a key part of the problem where a lot of experimentation with hyperparameters is needed. The number of layers, their sizes and different functions in the network needs to be examined. Once an optimal performance has been achieved it also needs to be compared with some baseline in order to evaluate whether this method is good in practice or not.

\subsection{Dataset}
Out of all datasets available not all are suited to the task. The dataset needs satisfy the following constraints: sufficiently large, naturally labelled, there is some underlying structure that can be learnt.

\subsection{Hyperparameters}
The hyperparameters for the ANN will in the end determine its performance. This is since the hyperparameters determine characteristics of the ANN - its shape, size and behaviour. Some of these hyperparameters that needs to be examined are described below:

\begin{itemize}
    \item Depth of network, how many layers the network needs
    \item Bredth of layers, how many nodes each layer has
    \item Size of embedding matrix
    \item Learning rate
\end{itemize}

\subsection{Baseline}
When evaluating the performance of the network one or more baselines are needed as reference points. It not sufficient to say that the model simply \textit{performs well}, if there is no context \textit{well} could mean just about anything. It is therefore important to have one more or baselines to serve as references.

\section{Scope}
This project will evaluate the possibilities of using deep learning to accurately be able to recommend users of a social platform that will be interested in new content posted on the platform.

\subsection{Limitations}
This project will be mostly scientific and will not focus on business applications of the problem.
\\\\
Due  to  technical  limitations  regarding  computing  power  and  available  data  the number of users that will be considered when recommending will be smaller than what would likely be desired in a commercial implementation. This  is  to  make  the  training  of  the  network  take  less  time  and  thereby make it possible to perform more experiments. The total number of users that will be considered will be fixed to contain the initial number of users and a buffer for new users. When adding a new user the model remains trained, and only needs to learn from the new user’s data.
\\\\
One issue that arises when dealing with data of this kind is the ethical concern regarding integrity. It is not hard to imagine that a system that could tell whether or not something is of interest to a specific person could be abused. The system could perhaps discover areas of interest that a person would have wished to keep private, this is just one example of potentially many problems. Due to time constraints, research into what is ethical will not be considered. 